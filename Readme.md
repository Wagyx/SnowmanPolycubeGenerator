# Polycube Generator in C

A 3D polycube generator based on the Computerphile video and [this repository](https://github.com/mikepound/opencubes). This version is written in C with some optimizations indicated below. Probably not the best implementation out there, but I wanted to try out some ideas.

## Compiling

```bash
make clean all
```

## Usage
The following returns the number of 3D polycubes of length 5:
```bash
./polycube_generator 5
```

A cache file can be generated by adding a filename at the end:
```bash
./polycube_generator 5 cubes5.dat
```
(Note: cache files currently aren't used in computation, though working code for reading them is "decompress()" in "key_compression.c")

## Algorithms

Basics:
- The core of this version uses sorted point lists as the basis of the generated cube "keys". The lowest coordinate is 1 in each direction.
- Before any computation is performed, we compute all relevant rotations of the initial point list, including versions expand the dimensions of the list beyond the original size.
- Rotations are reduced to 4 for any dimension set with a dimension not equal to the other 2 dimensions. All 24 rotations are used otherwise.
- Candidate points are determined on all faces of the initial list of length n-1. We sort the list and compare with the original point list to eliminate overlapping points.

This is "hashtable-less" implementation similar to that described by [presseyt](https://github.com/mikepound/opencubes/issues/11). The difference is checking if the removed point from the new polycube is the highest possible index in the polycube point list. When doing this in combination with removing all duplicate polycubes from the current "seed" shape, we are left with a unique set of generated cubes. Specific steps taken:
1. Start with polycube p. Extend by cube a to yield cube q (q = p + a).
2. Repeat step 1 for all possible additions of a.
3. Filter result of step 2 to only have unique values, saving the highest possible of the index of a in each q.
4. For each q, check if there is any possible r at a higher index of q that can be removed while remaining a polycube. Otherwise output q.

The advantage of this method is we don't need to recanonize the combinations. I started with a rather lazy implementation of steps 3 and 4:
1. Sort all generated q polycubes. Check the current q with the last q to eliminate duplicates.
2. Use a fast "number of neighbors" check to eliminate the majority of higher duplicate r indexes. Specifically, if a cube in q only has 1 neighbor, it can safely be removed.
3. If the neighbor check doesn't cancel the output, iteratively check all r values of index higher than a. We count the number of cubes that can be reached to make sure it is connected. 

## Performance

Using a rather dated FX-8350 processor with 6 computing threads on Ubuntu:
- n=11 in 3 seconds
- n=12 in 14 seconds
- n=13 in 117 seconds
- n=14 in 1119 seconds (19 minutes)
- n=15 in 9291 seconds (2 hours 35 minutes)

## Cache Files

The cache file format is structured as follows:
1. (1 byte) Length of the polycube
2. (n * key_size bytes) Polycube data

The encoding is a bit / face scheme somewhat similar to that described by presseyt. The cubes are described in discovery order based on the +xyz / -xyz faces. We skip the face of each cube on which it was discovered, and we skip the last cube because it will always be zeroes.

Note there will be exactly n-1 bits for a polycube of length n. This unambiguously describes the position of all cubes, making recovering the shape rather simple. It also could lead to further space reduction by noticing this data can only have a certain number of states, though I have yet to figure out a good way to do so.

For reference, the space utilization is 70.7 GB for n=15 with 9 bytes per polycube.
Still, I estimate n=18 has 42.1 TB required, but I doubt anybody would want to generate that much data.

Example n=4 (note "." designates the skipped bit):

    100000 100.00 100.00
    (this describes a straight line)

## Known Areas for Improvement
- qsort [can be slow](https://travisdowns.github.io/blog/2019/05/22/sorting.html), and it appears in critical areas (initial candidate sorting and filtering for duplicate removal). Maybe mergesort or hash methods could be faster.
- The main algorithm used here probably doesn't compete with [hidny's solution](https://github.com/mikepound/opencubes/issues/27).
- Command argument handling. Ability to select number of threads, use cache files, etc.
- Graph algorithms could probably improve the performance of the full connection check.

## License

[MIT](https://choosealicense.com/licenses/mit/)
